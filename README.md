# Single GPU LLM Model Training

Following the introduction of the transformer architecture in Attention is All You Need (2017), transformer based text generation models -- commonly refered to as Large Langage Models due to their large size (as of 2026, current best performing LLMs reach the $10^{11}$ number of parameters) - have become a key point of focus for engineers and researchers alike. One of the main 

The aim of this project is to build a Large Langage Model using the transformer architecture